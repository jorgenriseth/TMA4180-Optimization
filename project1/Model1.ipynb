{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# take in vector of unknowns, and build A matrix and c-vector\n",
    "def from_x_to_matrix(x):\n",
    "    n = int((-3 + np.sqrt(9+8*x.size))//2)\n",
    "    k = x.size - n\n",
    "    A = np.zeros((n, n))\n",
    "    c = np.zeros(n)\n",
    "    \n",
    "    #Insert first k coefficients into matrix\n",
    "    end = 0\n",
    "    for j in range(n):\n",
    "        start = end\n",
    "        end = start + n-j\n",
    "        A[j, j:] = x[start:end]\n",
    "        A[j+1:,j] = A[j, j+1:]\n",
    "    c = x[-n:] #Insert last n coefficients into vector\n",
    "    return A, c\n",
    "\n",
    "# Calculate g for single point z\n",
    "def gi(x, zi):\n",
    "    A, c = from_x_to_matrix(x)\n",
    "    return (zi-c).dot(A.dot(zi-c)) - 1\n",
    "\n",
    "# Calculate residual r for single point z\n",
    "def r(x, zi, wi):\n",
    "    return np.maximum(wi * gi(x, zi), 0)\n",
    "\n",
    "# Calculate residual vector\n",
    "def R(x, Z, W):\n",
    "    m, n = Z.shape\n",
    "    R = np.zeros(m)\n",
    "    for i in range(R.size):\n",
    "        R[i] = r(x, Z[i], W[i])\n",
    "    return R\n",
    "\n",
    "# Calculate objective function\n",
    "def f(x, Z, W):\n",
    "    m, n = Z.shape\n",
    "    return np.sum(R(x, Z, W)**2)\n",
    "\n",
    "# Calculate gradient of g for single point z\n",
    "def dgi(x, zi):\n",
    "    n = zi.size\n",
    "    dg = np.zeros(x.size)\n",
    "    end = 0\n",
    "    A, c = from_x_to_matrix(x)\n",
    "    v = zi - c\n",
    "    for j in range(n):\n",
    "        start = end\n",
    "        end = start + n - j\n",
    "        dg[start] = v[j]**2\n",
    "        dg[start+1:end] = 2 * v[j] * v[start+1:end]\n",
    "    dg[-n:] = -2 * A.dot(v)\n",
    "    return dg\n",
    "\n",
    "# Calculate gradient of residual r for single point z\n",
    "# h is the gi-value for the given point\n",
    "def dri(x, zi, wi, ri = None):\n",
    "    n = zi.size\n",
    "    dr = np.zeros(x.size)\n",
    "    if ri == None:\n",
    "        ri = r(x, zi, wi)\n",
    "    return (ri > 0) * dgi(x, zi) * wi\n",
    "\n",
    "# Calculate jacobian of residual vector R\n",
    "def jacobi(x, Z, W, g = None):\n",
    "    m, n = Z.shape\n",
    "    J = np.zeros((m, x.size))\n",
    "    for i in range(m):\n",
    "        J[i] = dri(x, Z[i], W[i], g)\n",
    "    return J\n",
    "\n",
    "# Calculate gradient of objective function\n",
    "def df(x, Z, W, g = None):\n",
    "    return 2 * (jacobi(x, Z, W, g).T).dot(R(x, Z, W))\n",
    "\n",
    "# Evaluate hi(x) for a matrix of columnvectors\n",
    "def G(X, A, c):\n",
    "    return (((X - c)  @ A) * (X-c)).sum(axis = 1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# RUN TEST ###################\n",
    "# Finite difference test of gradient\n",
    "def finite_difference_test(m = 10, n = 2):\n",
    "    print(\"Finite differences verification of gradient\")\n",
    "    x, Z, W = generate_random(m, n)\n",
    "    k = n*(n+1)//2\n",
    "    p = np.random.randn(n+k)\n",
    "    p = p/np.linalg.norm(p)\n",
    "    f0 = f(x, Z, W)\n",
    "    g = df(x, Z, W).dot(p)\n",
    "    if g == 0:\n",
    "        print(\"p: \\n\", p)\n",
    "        print(df(x, Z, W))\n",
    "\n",
    "    else:\n",
    "        print(\"g = %e\" %g)\n",
    "        for ep in 10.0**np.arange(2, -9, -1):\n",
    "            g_app = (f(x+ep*p, Z, W)-f0)/ep\n",
    "            error = abs(g_app-g)/abs(g)\n",
    "            print('ep = %e, error = %e, g_app = %e' % (ep,error, g_app))\n",
    "            \n",
    "\n",
    "# Generate a random set of points, weights, and unknowns\n",
    "def generate_random(m, n):\n",
    "    Z = np.random.randn(m*n).reshape(m, n)\n",
    "    W = np.random.choice([-1.0, 1.0], m)\n",
    "    x = np.array((1,0,1,0,0))\n",
    "    return x, Z, W   \n",
    "\n",
    "\n",
    "# Given A, b/c, evaluate function values for contourplot.\n",
    "def evaluate_function(func, A, b, xlim = (-5, 5), ylim = (-5, 5)):\n",
    "    x = np.linspace(*xlim, 101)\n",
    "    y = np.linspace(*ylim, 101)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    X = np.stack((xx.flatten(), yy.flatten())).T\n",
    "    return xx, yy, func(X, A, b).reshape(xx.shape)\n",
    "\n",
    "\n",
    "def optimize(x, Z, W, method, func, output = False, backtrack = True):\n",
    "    a, it, f = method(f, df, x, Z, W, backtrack)\n",
    "    print(\"Iterations: \", it)\n",
    "    A, b = from_x_to_matrix(a)\n",
    "    xx, yy, C = evaluate_function(func, A, b)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.contour(xx, yy, C, levels = [0])\n",
    "    ax1.plot(Z[W==1, 0], Z[W==1,1], 'o', color = 'b', label = \"+\")\n",
    "    ax1.plot(Z[W==-1, 0], Z[W==-1,1], 'x', color = 'r', label = \"-\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    if output:\n",
    "        print(\"W                Z                            W\")\n",
    "        Res = R(a, Z, W)\n",
    "        for i in range(m):\n",
    "            print(W[i], \" \", Z[i], \" \"*10,  Res[i])\n",
    "\n",
    "    return ax1\n",
    "\n",
    "\n",
    "# Minimize and visualize objective function values for a random set of points\n",
    "def optimize_random(m, n, method, func, output = False):\n",
    "    x, Z, W = generate_random(m, n)\n",
    "    a, it = method(f, df, x, Z, W)\n",
    "    print(\"Iterations: \", it)\n",
    "    A, b = from_x_to_matrix(a)\n",
    "    xx, yy, C = evaluate_function(func, A, b)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.contour(xx, yy, C, levels = [0])\n",
    "    ax1.plot(Z[W==1, 0], Z[W==1,1], 'o', color = 'b', label = \"+\")\n",
    "    ax1.plot(Z[W==-1, 0], Z[W==-1,1], 'x', color = 'r', label = \"-\")\n",
    "    ax1.legend()\n",
    "    if output:\n",
    "        print(\"W                Z                            W\")\n",
    "        Res = R(a, Z, W)\n",
    "        for i in range(m):\n",
    "            print(W[i], \" \", Z[i], \" \"*10,  Res[i])\n",
    "\n",
    "    return ax1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### ALGORITHMS #############################################\n",
    "def backtracking_line_search(f, gradf, p, x, Z, W):\n",
    "    ρ = 0.5\n",
    "    c = 0.05\n",
    "    α = 0.5\n",
    "    \n",
    "    ϕ_k = f(x + α * p, Z, W)\n",
    "    dF = gradf(x, Z, W)\n",
    "    it = 0\n",
    "    while (ϕ_k >= f(x, Z, W) + c * α * dF.dot(p) and it < 200):\n",
    "        α = ρ * α\n",
    "        ϕ_k = f(x + α * p, Z, W)\n",
    "        it += 1\n",
    "\n",
    "    return α\n",
    "\n",
    "def line_search(f, grad,  p, x, z, w):\n",
    "    \n",
    "    #Line search algorithm satisfying strong Wolfe conditions. P. 60 in NW\n",
    "    \n",
    "    alpha_prev = 0\n",
    "    alpha_max = 10000\n",
    "    alpha_curr = 2\n",
    "    \n",
    "    #Picked some random numbers\n",
    "    c1 = 1e-4\n",
    "    c2 = 0.5\n",
    "    \n",
    "    i = 1\n",
    "    while True:\n",
    "        #print(f(x + alpha_curr*p))\n",
    "        zero = np.array(np.zeros(len(x)))\n",
    "        dPhi_0 = np.dot(grad(zero, z, w),p)\n",
    "        dPhi_i = np.dot(grad(x + alpha_curr*p, z, w),p)\n",
    "        \n",
    "        #### FEIL I LINJEN UNDER?? Forskjell på ϕ og f\n",
    "        if (f(x + alpha_curr*p, z, w) > f(x, z, w) + c1*alpha_curr*dPhi_0) or ((f(x + alpha_curr*p, z, w) >= f(x + alpha_prev*p, z, w) and i > 1)):\n",
    "            return zoom(f, grad, p, x, c1, c2, alpha_curr, alpha_prev, z, w)\n",
    "        \n",
    "        if np.abs(dPhi_i) <= -c2*dPhi_0: \n",
    "            return alpha_curr\n",
    "        if dPhi_i >= 0:\n",
    "            return zoom(f, grad, p, x, c1, c2, alpha_prev, alpha_curr, z, w)\n",
    "        \n",
    "        alpha_prev = alpha_curr\n",
    "        alpha_curr = (alpha_curr + alpha_max)/2\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        #breaking point?\n",
    "        if i == 1000:\n",
    "            return alpha_curr\n",
    "        \n",
    "    \n",
    "    \n",
    "def interpolate(alpha2, alpha1, f, grad, p, x, z, w):\n",
    "    \n",
    "    #As described on p. 59 in NW. A cubic interpolation method to determine current step lenght within a interval    \n",
    "    \n",
    "    alpha_curr = alpha2\n",
    "    alpha_prev = alpha1\n",
    "    \n",
    "    phi_curr = f(x + alpha_curr*p, z, w)\n",
    "    phi_prev = f(x + alpha_prev*p, z, w)\n",
    "    \n",
    "    dPhi_curr = np.dot(grad(x + alpha_curr*p, z, w),p)\n",
    "    dPhi_prev = np.dot(grad(x + alpha_prev*p, z, w),p)\n",
    "    \n",
    "    d1 = dPhi_prev + dPhi_curr - 3*(phi_prev - phi_curr)/(alpha_prev - alpha_curr)\n",
    "    d2 = np.sign(alpha_curr - alpha_prev)*np.sqrt(d1**2 - dPhi_prev*dPhi_curr)\n",
    "    \n",
    "    \n",
    "    return alpha_curr - (alpha_curr - alpha_prev)*(dPhi_curr + d2 - d1)/(dPhi_curr - dPhi_prev + 2*d2)\n",
    "\n",
    "    \n",
    "def zoom(f, grad, p, x, c1, c2, Alpha_high, Alpha_low, z, w):\n",
    "    \n",
    "    #Zoom algorithm as described on p. 61 in NW. Used for finding optimal step lenght\n",
    "    alpha_low = Alpha_low\n",
    "    alpha_high = Alpha_high\n",
    "    \n",
    "    i = 0\n",
    "    tol = 10**-4\n",
    "    alpha_curr = alpha_high\n",
    "    alpha_prev = alpha_low\n",
    "    while True:\n",
    "        #Interpolate, choose alpha_j between hi & low\n",
    "        #alpha_j = (alpha_high + alpha_low)/2\n",
    "        \n",
    "        if (np.abs(alpha_high - alpha_low) < tol):\n",
    "            return alpha_high\n",
    "        alpha_j = interpolate(alpha_high, alpha_low, f, grad, p, x, z, w)\n",
    "        #print(alpha_j)\n",
    "        zero = np.array(np.zeros(len(x)))\n",
    "        phi_j = f(x + alpha_j*p, z, w)\n",
    "        phi_low = f(x + alpha_low*p, z, w)\n",
    "        \n",
    "        dPhi_0 = np.dot(grad(x, z, w),p) # Bør være grad i x\n",
    "        \n",
    "        \n",
    "        if (phi_j > f(x, z, w) + c1*alpha_j*dPhi_0) or (phi_j >= phi_low):\n",
    "            alpha_high = alpha_j\n",
    "        else:\n",
    "            dPhi_i = np.dot(grad(x + alpha_j*p, z, w),p)\n",
    "            if np.abs(dPhi_i) <= -c2*dPhi_0:\n",
    "                return alpha_j\n",
    "            if dPhi_i*(alpha_high - alpha_low) >= 0:\n",
    "                alpha_high = alpha_low\n",
    "            alpha_low = alpha_j\n",
    "        \n",
    "        #breaking point\n",
    "        i += 1\n",
    "        if i == 1000:\n",
    "            #print(alpha_j)\n",
    "            return alpha_j\n",
    "\n",
    "\n",
    "# Optimization algorithms\n",
    "def steepest_descent(f, grad, x0, Z, W, backtrack = True, tol = 1e-3, output = False):\n",
    "    p = -df(x0, Z, W)\n",
    "    x_k = x0\n",
    "    it = 0\n",
    "    \n",
    "    while np.linalg.norm(p) > tol and it < 10000:\n",
    "        \n",
    "        if backtrack:\n",
    "            α = backtracking_line_search(f, grad, p, x_k, Z, W)\n",
    "        else:\n",
    "            α = line_search(f, grad, p, x_k, Z, W)\n",
    "            \n",
    "        x_k = x_k + α * p\n",
    "        p = -df(x_k, Z, W)\n",
    "        it += 1\n",
    "        \n",
    "        if it % 500 == 0:\n",
    "            print(\"\\niter:\", it)\n",
    "            print(\"α =\", α)\n",
    "            print(\"f(x) =\", f(x_k, Z, W))\n",
    "            print(\"\\n\")\n",
    "    return x_k, it, f(x_k, Z, W)\n",
    "\n",
    "\n",
    "# Optimization algorithm\n",
    "def bfgs_method(f, grad, x0, Z, W, backtrack = True, tol = 1e-3):\n",
    "    m, n = Z.shape\n",
    "    k = n*(n+1)//2\n",
    "\n",
    "    I = np.identity(n + k)\n",
    "    H = I\n",
    "    \n",
    "    x_k = x0\n",
    "    dF = grad(x_k, Z, W)\n",
    "    \n",
    "    it = 0\n",
    "    while np.linalg.norm(dF) > tol and it < 10000:\n",
    "        dF = grad(x_k, Z, W)\n",
    "        \n",
    "        p_k = - H.dot(dF)\n",
    "        p_k = p_k/np.linalg.norm(p_k)\n",
    "        \n",
    "        if backtrack:\n",
    "            α_k = backtracking_line_search(f, grad, p_k, x_k, Z, W)\n",
    "        else:\n",
    "            α_k = line_search(f, grad, p_k, x_k, Z, W)\n",
    "        \n",
    "        x_next = x_k + α_k * p_k\n",
    "        dF_next = grad(x_next, Z, W)\n",
    "        \n",
    "        s_k = x_next - x_k\n",
    "        y_k = dF_next - dF\n",
    "        \n",
    "        # Check if \"reboot\" is needed\n",
    "        if s_k.dot(y_k) == 0:\n",
    "            H = I\n",
    "            continue\n",
    "            \n",
    "        # computing rho (6.14 in NW)\n",
    "        ρ_k = 1/(np.dot(s_k,y_k))\n",
    "        \n",
    "        \n",
    "        H = (I - ρ_k * s_k * y_k.T) @ H @ (I - ρ_k * y_k * s_k.T) + ρ_k * s_k * s_k.T\n",
    "        \n",
    "        it += 1\n",
    "        \n",
    "        x_k = x_next\n",
    "        dF = dF_next\n",
    "        \n",
    "        # Print progress\n",
    "        if it % 200 == 0:\n",
    "            print(\"\\niter:\", it)\n",
    "            print(\"α =\", α_k)\n",
    "            print(\"f(x) =\", f(x_k, Z, W))\n",
    "            print(\"\\n\")\n",
    "            \n",
    "    return x_k, it, f(x_k, Z, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iter: 500\n",
      "α = 0.00390625\n",
      "f(x) = 0.0466750282898\n",
      "\n",
      "\n",
      "\n",
      "iter: 1000\n",
      "α = 0.00390625\n",
      "f(x) = 0.0385170138274\n",
      "\n",
      "\n",
      "\n",
      "iter: 1500\n",
      "α = 0.00390625\n",
      "f(x) = 0.0350248006118\n",
      "\n",
      "\n",
      "\n",
      "iter: 2000\n",
      "α = 0.001953125\n",
      "f(x) = 0.0310860564051\n",
      "\n",
      "\n",
      "\n",
      "iter: 2500\n",
      "α = 0.001953125\n",
      "f(x) = 0.0290361155649\n",
      "\n",
      "\n",
      "\n",
      "iter: 3000\n",
      "α = 0.00390625\n",
      "f(x) = 0.0278030306731\n",
      "\n",
      "\n",
      "\n",
      "iter: 3500\n",
      "α = 0.001953125\n",
      "f(x) = 0.0268496615622\n",
      "\n",
      "\n",
      "\n",
      "iter: 4000\n",
      "α = 0.001953125\n",
      "f(x) = 0.0260455000711\n",
      "\n",
      "\n",
      "\n",
      "iter: 4500\n",
      "α = 0.00390625\n",
      "f(x) = 0.0253258049476\n",
      "\n",
      "\n",
      "\n",
      "iter: 5000\n",
      "α = 0.00390625\n",
      "f(x) = 0.0247305198595\n",
      "\n",
      "\n",
      "\n",
      "iter: 5500\n",
      "α = 0.001953125\n",
      "f(x) = 0.0241908451096\n",
      "\n",
      "\n",
      "\n",
      "iter: 6000\n",
      "α = 0.001953125\n",
      "f(x) = 0.0236991163821\n",
      "\n",
      "\n",
      "\n",
      "iter: 6500\n",
      "α = 0.001953125\n",
      "f(x) = 0.0232485138763\n",
      "\n",
      "\n",
      "\n",
      "iter: 7000\n",
      "α = 0.001953125\n",
      "f(x) = 0.0228340592656\n",
      "\n",
      "\n",
      "\n",
      "iter: 7500\n",
      "α = 0.001953125\n",
      "f(x) = 0.0224512758631\n",
      "\n",
      "\n",
      "\n",
      "iter: 8000\n",
      "α = 0.00390625\n",
      "f(x) = 0.0220970331902\n",
      "\n",
      "\n",
      "\n",
      "iter: 8500\n",
      "α = 0.001953125\n",
      "f(x) = 0.0217674697657\n",
      "\n",
      "\n",
      "\n",
      "iter: 9000\n",
      "α = 0.00390625\n",
      "f(x) = 0.0214607158166\n",
      "\n",
      "\n",
      "\n",
      "iter: 9500\n",
      "α = 0.001953125\n",
      "f(x) = 0.0211735405608\n",
      "\n",
      "\n",
      "\n",
      "iter: 10000\n",
      "α = 0.001953125\n",
      "f(x) = 0.0209050328506\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-865decfc1947>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimize_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteepest_descent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-68e07a3aa05d>\u001b[0m in \u001b[0;36moptimize_random\u001b[0;34m(m, n, method, func, output)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moptimize_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iterations: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_x_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "m, n = (10, 2)\n",
    "optimize_random(m, n, steepest_descent, G, output = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
